\title{CS 383 - Machine Learning}
\author{
        Assignment 1 - Dimensionality Reduction
}
\date{}
\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}


\begin{document}
\maketitle

\section{(25pts) Theory Questions}
\begin{enumerate}


\begin{center}
X = 
$
 \begin{bmatrix}
	-2 & 1\\
	-5 & -4\\	
	-3 & 1\\
	0 & 3\\
	-8 & 11\\
	-2 & 5\\
	1 & 0\\
	5 & -1\\
	-1 & -3\\
	6 & 1\\
	
\end{bmatrix}
$
Y = 
$
 \begin{bmatrix}
	1\\
	1\\
	1\\
	1\\
	1\\
	0\\
	0\\
	0\\
	0\\
	0\\
\end{bmatrix}
$
\end{center}
	\begin{enumerate}
	\item Compute the average entropy of the class label in the subsets created by splitting the dataset based on the value of the first feature.  You may assume that the features are \emph{categoricial}.  (5pts).
	
	p_{-2} = 1 \hspace{1cm}      	n_{-2} = 1\\ 
	p_{-5} = 1 \hspace{1cm}      	n_{-5} = 0\\ 
	p_{-3} = 1 \hspace{1cm}      	n_{-3} = 0 \\ 
	p_{0} = 1 \hspace{1cm}      	n_{0} = 0\\ 
	p_{-8} = 1 \hspace{1cm}      	n_{-8} = 0\\ 
	p_{-2} = 1 \hspace{1cm}      	n_{-2} = 1\\ 
	p_{1} = 0 \hspace{1cm}      	n_{1} = 1\\ 
	p_{5} = 0 \hspace{1cm}      	n_{5} = 1\\ 
	p_{-1} = 0 \hspace{1cm}      	n_{-1} = 1\\ 
	p_{6} = 0 \hspace{1cm}      	n_{6} = 1\\ 
	

	$$	E(H(A)) \sum_{i=1}^{K} \frac{p_{i}+n_{i}}{p + n} H \left (\frac{p_{i}}{p_{i}+n_{i}} , \frac{n_{i}}{p_{i}+n_{i}}\right)$$ \\
	
	Feature 1 (first index): \\
	
	$$	E(H(1)) \sum_{i=1}^{1} \frac{1+1}{5+5} * \left (-\frac{1}{1+1}log_{2} \left(\frac{1}{2}\right) + \frac{-1}{1+1}log_{2}\left(\frac{1}{2}\right)\right) = 0.2 $$ \\
	
	
	Feature 1 (second index): \\
	
	$$	E(H(1)) \sum_{i=2}^{2} \frac{1}{10} * \left (-\frac{1}{1}log_{2} \left(\frac{1}{1}\right) + \frac{-0}{1}log_{2}\left(\frac{0}{1}\right)\right) = 0 $$ \\
	
	Feature 1 (Summation): \\
	
	$$	E(H(1)) \sum_{i=1}^{10} \frac{p_{i}+n_{i}}{p + n} * \left (-\frac{p_{i}}{p_{i}+n_{i}}log_{2} \left(\frac{p_{i}}{p_{i}+n_{i}}\right) + \frac{n_{i}}{p_{i}+n_{i}}log_{2}\left(\frac{p_{i}}{p_{i}+n_{i}}\right)\right) = 0.4 $$ \\
	
	
	
	
	\item Now make the same computation, but if we created subsets using the second feature! (5pts)
	
	p_{1} = 2 \hspace{1cm}      	n_{1} = 1\\ 
	p_{-4} = 1 \hspace{1cm}      	n_{-4} = 0\\ 
	p_{1} = 2 \hspace{1cm}      	n_{1} = 1 \\ 
	p_{3} = 1 \hspace{1cm}      	n_{3} = 0\\ 
	p_{11} = 1 \hspace{1cm}      	n_{11} = 0\\ 
	p_{5} = 0 \hspace{1cm}      	n_{5} = 1\\ 
	p_{0} = 0 \hspace{1cm}      	n_{0} = 1\\ 
	p_{-1} = 0 \hspace{1cm}      	n_{-1} = 1\\ 
	p_{-3} = 0 \hspace{1cm}      	n_{-3} = 1\\ 
	p_{1} = 2 \hspace{1cm}      	n_{1} = 1\\ 
	

	$$	E(H(A)) \sum_{i=1}^{K} \frac{p_{i}+n_{i}}{p + n} H \left (\frac{p_{i}}{p_{i}+n_{i}} , \frac{n_{i}}{p_{i}+n_{i}}\right)$$ \\
	
	Feature 2 (first index): \\
	
	$$	E(H(1)) \sum_{i=1}^{1} \frac{3}{10} * \left (-\frac{2}{3}log_{2} \left(\frac{2}{3}\right) + \frac{-1}{3}log_{2}\left(\frac{1}{3}\right)\right) = 0.275 $$ \\
	
	
	Feature 2 (second index): \\
	
	$$	E(H(1)) \sum_{i=2}^{2} \frac{1}{10} * \left (-\frac{1}{1}log_{2} \left(\frac{1}{1}\right) + \frac{-0}{1}log_{2}\left(\frac{0}{1}\right)\right) = 0 $$ \\
	
	Feature 2 (Summation): \\
	
	$$	E(H(2)) \sum_{i=1}^{10} \frac{p_{i}+n_{i}}{p + n} * \left (-\frac{p_{i}}{p_{i}+n_{i}}log_{2} \left(\frac{p_{i}}{p_{i}+n_{i}}\right) + \frac{n_{i}}{p_{i}+n_{i}}log_{2}\left(\frac{p_{i}}{p_{i}+n_{i}}\right)\right) = 0.826 $$ \\
	\item Which feature is more discriminating based on results in Part (a) (5pt)?\\
	\textbf{The first feature is more discriminating}
	\item What are the principle components of the matrix $X$?  For this (and the next) part you may assume that the features are \emph{continuous}.  Make sure the principle components  are all unit length.   You \textbf{MAY} use a utility function like \emph{eig} or \emph{svd} to determine these. (5pts)
	
	\centering{$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$} \\
	
		\textbf {To get the principle components, we need the eigen vectors, so we get them after using our original matrix to compute the covariance matrix. The resulting eigenvectors are: } \\
		
		\begin{bmatrix}
        -0.71636104 & 0.69772979 \\
        -0.69772979 & -0.71636104
        \end{bmatrix}


	\item If we were to project our data down to 1-D using the principle component, what would the new data matrix $X$ be? (5pts)
	
	\textbf{by multiplying the standardized data by our eigenvectors, the new data matrix would be} \\
	
	X =	\begin{bmatrix}
        -0.1117808\\ 0.23419233 \\ -0.27394203 \\ -0.12044108 \\ -2.74966176 \\ -0.77774625 \\ 0.54119423 \\ 1.35633049 \\ 0.71634587 \\ 1.18550899
        \end{bmatrix}
	
	
	\end{enumerate}

\end{enumerate}


\newpage
\section{(30pts) Dimensionality Reduction via PCA}\label{pca}

\begin{figure}[htp]
    \centering
    \includegraphics[width=20cm]{Figure_1.png}
    \caption{PCA Visualization}
    \label{fig:PCA Visualization}
\end{figure}
\end{document}